<p align="center">
  <a href="" rel="noopener">
 <img width=210px height=200px src="https://media-exp1.licdn.com/dms/image/C4D0BAQG7OjFosqn9dA/company-logo_200_200/0/1625776698161?e=2159024400&v=beta&t=1yvuOj8Q0MbxulP1hEfwjY5U55aU8VdBW77FNJISzpI" alt="Project logo"></a>
</p>

<h3 align="center">Bootcamp Engenheiro(a) de Dados Cloud - IGTI - MÃ³dulo 2</h3>

<div align="center">

[![Status](https://img.shields.io/badge/status-active-success.svg)]()
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](/LICENSE)

</div>

---

## ğŸ“ ConteÃºdo

- [Arquitetura](#architeture)
- [Autor](#authors)

## Arquitetura Orientada a Eventos <a name = "about"></a>



![k8s](img/Arquitetura-Igti.png)


## â›ï¸ Built Using <a name = "built_using"></a>

- EKS
- Kafka Strimzi Operator
- Apache Pinot
- KsqlDB
- Python 

## âœï¸ Authors <a name = "authors"></a>

- [@carlosbpy](https://github.com/carlosbpy)

Objetivos: 

ïƒ¼ Utilizar os principais serviÃ§os de nuvem para Engenharia de Dados;
ïƒ¼ Utilizar o Kubernetes como Gerenciador de Containers;
ïƒ¼ Ingerir dados em real time no Apache Kafka;
ïƒ¼ Implementar um pipeline de processamento de Big Data em real time;
ïƒ¼ Realizar Processamento de dados utilizando Ksqldb;
ïƒ¼ Disponibilizar dados no Apache Pinot.

Enunciado:

VocÃª Ã© Engenheiro(a) de Dados e precisa realizar a migraÃ§Ã£o de uma tabela relacionada a clientes,
que Ã© altamente requisitada pela Ã¡rea de negÃ³cios para, assim, realizar anÃ¡lises comportamentais
dos clientes. O gestor de sua Ã¡rea iniciou um projeto de migraÃ§Ã£o para que esses dados sejam
disponibilizados em tempo real, e vocÃª serÃ¡ responsÃ¡vel por construir uma pipeline em real time
para disponibilizar esses dados no DW. VocÃª precisarÃ¡ realizar o processamento utilizando
ferramenta adequada e disponibilizar o dado para consultas dos usuÃ¡rios de negÃ³cios e analistas de
BI.
Para a realizaÃ§Ã£o desta atividade, recomenda-se o uso dos serviÃ§os AWS.


Atividades:

VocÃª deverÃ¡ desempenhar as seguintes atividades:

1. Crie um RDS Postgres que tenha acesso pÃºblico liberado;
2. Clonar o projeto https://github.com/carlosbpy/igti-k8s-exercise;
3. Ingerir dados da tabela de customers para o postgres;
4. Deployar EKS na AWS ou utilizar o Minikube;
5. Migrar tabela de customers para a estrutura do Kafka;
6. Realizar o processamento dos dados da tabela de customers utilizando o KsqlDB;
7. Disponibilizar dados da tabela de customers em real time para o Apache Pinot.